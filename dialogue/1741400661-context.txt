

Let's design a search tool that builds on url_fetch.py it will take keyword or list of keywords, use the GOogle Search JSON API (assume I have keys) to retrieve 5 results. Then it will use the screencapture flow from url_fetch to take screenshots of the first 5 results. Finally, it will use the LLM logic from composer.py to send the image to an LLM for summarization. 


Directory Structure:
.
├── commands
│   ├── 1741374726.sh
│   ├── 1741374768.sh
│   ├── 1741377087.sh
│   └── 1741395755.sh
├── composer.py
├── conductor.py
├── patches
│   ├── 1741374726.patch
│   ├── 1741374768.patch
│   ├── 1741377087.patch
│   └── 1741395755.patch
├── pyvenv.cfg
├── search.py
└── url_fetch.py

3 directories, 13 files


# Content of composer.py:
import os
import json
import time
import base64
from typing import Dict, Any, Optional, List
from flask import Flask, request, jsonify
from openai import OpenAI
from anthropic import Anthropic

app = Flask(__name__)

# Load Configuration
def load_config():
    with open('config.json', 'r') as f:
        return json.load(f)

config = load_config()
openai_api_key = config.get("openai_api_key")
anthropic_api_key = config.get("anthropic_api_key")
openrouter_api_key = config.get("openrouter_api_key")

# Initialize Clients
openai_client = None
anthropic_client = None
openrouter_client = None

if openai_api_key:
    openai_client = OpenAI(api_key=openai_api_key)

if anthropic_api_key:
    anthropic_client = Anthropic(api_key=anthropic_api_key)

if openrouter_api_key:
    openrouter_client = OpenAI(base_url="https://openrouter.ai/api/v1",api_key=openrouter_api_key)

@app.route('/api/generate', methods=['POST'])
def generate():
    data = request.json
    model = data.get('model', 'gpt-4o')
    messages = data.get('messages', [])
    max_tokens = data.get('max_tokens', 1500)
    temperature = data.get('temperature', 0.7)
    provider = data.get('provider', 'openrouter')  # Default to OpenAI if not specified
    
    try:
        if provider == 'openai' and openai_client:
            response = openai_client.chat.completions.create(
                model=model,
                messages=messages,
                max_tokens=max_tokens,
                temperature=temperature
            )
            return jsonify({
                'success': True,
                'content': response.choices[0].message.content.strip(),
                'model': model,
                'provider': 'openai'
            })
            
        elif provider == 'anthropic' and anthropic_client:
            # Convert OpenAI message format to Anthropic format
            anthropic_messages = []
            for msg in messages:
                anthropic_messages.append({
                    "role": msg["role"],
                    "content": msg["content"]
                })
            
            response = anthropic_client.messages.create(
                model="claude-3-7-sonnet-20250219" if model == "default" else model,
                max_tokens=max_tokens,
                messages=anthropic_messages
            )
            
            return jsonify({
                'success': True,
                'content': response.content[0].text.strip(),
                'model': model,
                'provider': 'anthropic'
            })
        elif provider == 'openrouter' and openrouter_client:
            response = openrouter_client.chat.completions.create(
                model=model,
                messages=messages,
                max_tokens=max_tokens,
                temperature=temperature
            )
            return jsonify({
                'success': True,
                'content': response.choices[0].message.content.strip(),
                'model': model,
                'provider': 'openrouter'
            })
        else:
            return jsonify({
                'success': False,
                'error': f"Provider '{provider}' not available or no valid API keys found."
            }), 400
            
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/health', methods=['GET'])
def health_check():
    available_providers = []
    if openai_client:
        available_providers.append('openai')
    if anthropic_client:
        available_providers.append('anthropic')
    if anthropic_client:
        available_providers.append('openrouter')
        
    return jsonify({
        'status': 'ok',
        'available_providers': available_providers
    })

if __name__ == '__main__':
    port = int(os.environ.get('PORT', 5555))
    app.run(host='0.0.0.0', port=port, debug=True)


# Content of url_fetch.py:
import argparse
import time
import os
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By
from urllib.parse import urlparse

def capture_webpage(url, output_type='screenshot', output_file=None):
    # Set up headless Chrome
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--start-maximized")  # Ensures full-width capture
    driver = webdriver.Chrome(options=chrome_options)

    try:
        # Load the page
        driver.get(url)

        # Wait for page to load (adjust timeout and conditions as needed)
        wait = WebDriverWait(driver, 10)
        wait.until(EC.presence_of_element_located((By.TAG_NAME, "body")))

        # Additional wait to allow AJAX content to load
        time.sleep(5)

        # Create 'captures' directory if it doesn't exist
        os.makedirs('captures', exist_ok=True)

        # Generate filename from URL
        if output_file:
            filename = output_file
        else:
            parsed_url = urlparse(url)
            filename = parsed_url.netloc + parsed_url.path.replace('/', '_')
            if not filename.strip():
                filename = 'homepage'

        if output_type == 'screenshot':
            # Get the total height of the page
            total_height = driver.execute_script("return document.body.scrollHeight")
            
            # Set window size to capture full page
            driver.set_window_size(1920, total_height)
            
            # Take a full page screenshot
            screenshot_path = os.path.join('captures', f"{filename}.png")
            driver.save_screenshot(screenshot_path)
            print(f"Screenshot saved as {screenshot_path}")
            return screenshot_path
        elif output_type == 'content':
            # Get full page content
            page_content = driver.page_source
            content_path = os.path.join('captures', f"{filename}.html")
            with open(content_path, "w", encoding="utf-8") as f:
                f.write(page_content)
            print(f"Full page content saved as {content_path}")
            return content_path

    finally:
        driver.quit()

def main():
    parser = argparse.ArgumentParser(description="Capture webpage screenshot or content")
    parser.add_argument("url", help="URL to capture")
    parser.add_argument("--type", choices=['screenshot', 'content'], default='screenshot',
                        help="Type of capture: 'screenshot' or 'content' (default: screenshot)")
    parser.add_argument("-o", "--output", help="Output file name")
    
    args = parser.parse_args()
    
    capture_webpage(args.url, args.type, args.output)

if __name__ == "__main__":
    main()


# Content of pyvenv.cfg:
home = /opt/homebrew/opt/python@3.12/bin
include-system-site-packages = false
version = 3.12.9
executable = /opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/bin/python3.12
command = /opt/homebrew/opt/python@3.12/bin/python3.12 -m venv /Users/steppe/reich


# Content of conductor.py:
import os
import time
import argparse
from PIL import Image
import io
import requests
import json
import sys
import base64
from pathlib import Path
from mimetypes import guess_type
import re
import glob
import subprocess

# Import the diarize module from the current project
import diarize
# import utility to 
from url_fetch import capture_webpage

# Constants
DIALOGUE_DIR = "dialogue/"
PREAMBLE_FILE = "preamble.txt"
EXCLUDE_FILE = "exclude.txt"
GENERATED_DIR = "generated/"
SERVER_URL = "http://localhost:5555/api"  # Default server URL

def get_epoch_time():
    return str(int(time.time()))

def encode_image(image_path):
    mime_type, _ = guess_type(image_path)
    if mime_type is None:
        mime_type = 'application/octet-stream'

    with open(image_path, "rb") as image_file:
        base64_encoded_data = base64.b64encode(image_file.read()).decode('utf-8')
    return f"data:{mime_type};base64,{base64_encoded_data}"

def process_image(image_path, max_height=7999):
    with Image.open(image_path) as img:
        width, height = img.size
        
        # Check if image needs to be split
        if height > width * 4/3 and height > max_height:
            pieces = []
            for i in range(0, height, max_height):
                box = (0, i, width, min(i+max_height, height))
                piece = img.crop(box)
                
                # Convert piece to base64
                buffer = io.BytesIO()
                piece.save(buffer, format="PNG")
                encoded_piece = base64.b64encode(buffer.getvalue()).decode('utf-8')
                pieces.append(f"data:image/png;base64,{encoded_piece}")
            
            return pieces
        else:
            # If image doesn't need splitting, return it as is
            return [encode_image(image_path)]

def save_prompt(prompt_text, final_context):
    epoch_time = get_epoch_time()
    prompt_file = os.path.join(DIALOGUE_DIR, f"{epoch_time}-prompt.txt")
    context_file = os.path.join(DIALOGUE_DIR, f"{epoch_time}-context.txt")
    
    try:
        with open(prompt_file, 'w') as f:
            f.write(prompt_text)

        with open(context_file, 'w') as f:
            f.write(final_context)

    except Exception as e:
        print(f"Error saving files: {e}")
    
    return epoch_time, prompt_file, context_file

def load_preamble():
    with open(PREAMBLE_FILE, 'r') as f:
        return f.read().strip()

def load_exclusions():
    if os.path.exists(EXCLUDE_FILE):
        with open(EXCLUDE_FILE, 'r') as f:
            return [line.strip() for line in f if line.strip()]
    return []

def generate_directory_structure(root_dir, exclude_file):
    exclude_list = []
    if os.path.exists(exclude_file):
        with open(exclude_file, 'r') as f:
            exclude_list = [line.strip() for line in f.readlines()]

    # Construct the tree command with excludes
    exclude_params = []
    for item in exclude_list:
        exclude_params.append(f"-I '{item}'")

    exclude_str = ' '.join(exclude_params)
    command = f"tree {root_dir} {exclude_str} --prune"
    
    # Execute the tree command
    result = subprocess.run(command, shell=True, capture_output=True, text=True)
    
    return result.stdout

def gather_context(exclusions):
    # Generate the directory structure
    dir_structure = generate_directory_structure('.', EXCLUDE_FILE)
    context = f"Directory Structure:\n{dir_structure}"
    
    all_files = [f for f in glob.glob("**/*", recursive=True) if os.path.isfile(f)]
    exclude_files = []
    exclude_dirs = []
    for pattern in exclusions:
        if '.' in pattern:
            exclude_files.append(pattern)
        else:
            exclude_dirs.append(pattern)

    # Append contents of files to the context, considering exclusions
    for file in all_files:
        if any(file.startswith(excluded_dir) for excluded_dir in exclude_dirs):
            continue
        if any(glob.fnmatch.fnmatch(file, pattern) for pattern in exclude_files):
            continue
        with open(file, 'r', errors="ignore") as f:
            context += f"\n\n# Content of {file}:\n"
            context += f.read()
    return context

def gather_message_history():
    files = sorted(glob.glob(os.path.join(DIALOGUE_DIR, "*.txt")), key=os.path.getmtime)
    summaries = [f for f in files if "summary" in f]
    prompts = [f for f in files if "prompt" in f]
    responses = [f for f in files if "response" in f]

    message_history = []

    if summaries:
        with open(summaries[-1], 'r') as f:
            message_history.append({"role": "assistant", "content": f.read().strip()})

    for p, r in zip(prompts, responses):
        with open(p, 'r') as f:
            message_history.append({"role": "user", "content": f.read().strip()})
        with open(r, 'r') as f:
            message_history.append({"role": "assistant", "content": f.read().strip()})

    return message_history

def send_request_to_server(prompt, image_paths=None, server_url=SERVER_URL, provider="openrouter", model="claude-3-7-sonnet-20250219"):
    message_history = gather_message_history()
    
    if image_paths:
        for image_path in image_paths:
            image_pieces = process_image(image_path)
            for piece in image_pieces:
                if provider == "anthropic":
                    message_history.append({
                        "role": "user",
                        "content": [
                            {
                                "type": "image",
                                "source": {
                                    "type": "base64",
                                    "media_type": "image/png",
                                    "data": piece.split(",", 1)[1]
                                }
                            }
                        ]
                    })
                else:
                    message_history.append({
                        "role": "user",
                        "content": [
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": piece
                                }
                            }
                        ]
                    })
    
    # Add the text prompt last
    message_history.append({"role": "user", "content": prompt})

    # Prepare request data
    request_data = {
        "messages": message_history,
        "max_tokens": 1500,
        "temperature": 0.7,
        "provider": provider,
        "model": model
    }
    
    # Send request to server
    try:
        response = requests.post(
            f"{server_url}/generate",
            json=request_data,
            headers={"Content-Type": "application/json"}
        )
        
        # Handle response
        if response.status_code == 200:
            result = response.json()
            if result.get("success", False):
                return result.get("content", "")
            else:
                error_msg = result.get("error", "Unknown error")
                raise Exception(f"Server error: {error_msg}")
        else:
            raise Exception(f"HTTP error: {response.status_code} - {response.text}")
            
    except requests.exceptions.RequestException as e:
        raise Exception(f"Connection error: {str(e)}")

def guess_image_mime_type(encoded_image):
    """Guess the MIME type of the image from the data URL"""
    if encoded_image.startswith("data:image/jpeg"):
        return "image/jpeg"
    elif encoded_image.startswith("data:image/png"):
        return "image/png"
    elif encoded_image.startswith("data:image/gif"):
        return "image/gif"
    elif encoded_image.startswith("data:image/webp"):
        return "image/webp"
    else:
        return "application/octet-stream"  # Default to binary data if unknown
    
def main():
    """Main function to run the Reich client."""
    Path(DIALOGUE_DIR).mkdir(exist_ok=True)
    
    # Parse command line arguments
    parser = argparse.ArgumentParser(description="Reich client for AI text generation")
    parser.add_argument('-f', '--file', default='prompt', help='File path to read prompt from')
    parser.add_argument("-i", "--images", nargs='+', required=False, help="Image files to send along with the prompt")
    parser.add_argument("-u", "--urls", nargs='+', required=False, help="URLs to capture screenshots from")
    parser.add_argument("-s", "--server", default=SERVER_URL, help=f"Server URL (default: {SERVER_URL})")
    parser.add_argument("-p", "--provider", default="openrouter", choices=["auto", "openai", "openrouter", "anthropic"])
    parser.add_argument("-m", "--model", default="anthropic/claude-3.7-sonnet", help="Model to use")

    args = parser.parse_args()
    
    # Process user input
    if args.file:
        with open(os.path.expanduser(args.file), 'r') as file:
            user_prompt = file.read()
    else:
        user_prompt = input("\nEnter your prompt: ")
    
    # Capture screenshots if URLs are provided
    captured_images = []
    if args.urls:
        for url in args.urls:
            screenshot_path = capture_webpage(url)
            captured_images.append(screenshot_path)
    
    # Combine captured screenshots with provided images
    image_paths = (args.images or []) + captured_images
    
    # Load context
    preamble = load_preamble() if os.path.exists(PREAMBLE_FILE) else ""
    exclusions = load_exclusions()
    context = gather_context(exclusions)
    
    # Prepare final prompt with context
    final_prompt = f"{preamble}\n\n{user_prompt}\n\n{context}"
    epoch_time, prompt_file, context_file = save_prompt(user_prompt, final_context=final_prompt)

    try:
        # Send request to AI server
        response_text = send_request_to_server(
            prompt=final_prompt, 
            image_paths=image_paths,
            server_url=args.server,
            provider=args.provider,
            model=args.model
        )
        
        # Save the response
        response_file = os.path.join(DIALOGUE_DIR, f"{epoch_time}-response.txt")
        with open(response_file, 'w') as f:
            f.write(response_text)
        
        # Extract and save code blocks if present
        code_blocks = re.findall(r'```(.*?)```', response_text, re.DOTALL)
        if code_blocks:
            Path(GENERATED_DIR).mkdir(exist_ok=True)
            for i, code_block in enumerate(code_blocks):
                code_block = code_block.strip()
                if code_block.startswith('python'):
                    extension = '.py'
                    content = code_block.split('\n', 1)[1] if '\n' in code_block else code_block
                elif code_block.startswith('javascript'):
                    extension = '.js'
                    content = code_block.split('\n', 1)[1] if '\n' in code_block else code_block
                else:
                    extension = '.txt'
                    content = code_block
                    
                filename = os.path.join(GENERATED_DIR, f"{epoch_time}_{i}{extension}")
                with open(filename, 'w') as file:
                    file.write(content)
        
        # Print the response
        print("\n" + "="*50)
        print("RESPONSE:")
        print("="*50)
        print(response_text)
        
        # Update conversation summary
        if 'diarize' in sys.modules:
            diarize.summarize_conversation()
            
    except Exception as e:
        print(f"Error in processing: {e}")
        return 1
        
    return 0

if __name__ == "__main__":
    sys.exit(main())


# Content of search.py:
# search.py
import os
import sys
import argparse
from pathlib import Path
import glob
import json
from datetime import datetime
from typing import List, Dict, Any

# RAG components
from langchain_community.embeddings import OpenAIEmbeddings, HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.documents import Document

def load_config():
    """Load API keys from config file"""
    with open('config.json', 'r') as f:
        return json.load(f)

def initialize_embeddings(config):
    """Initialize embeddings model based on available API keys"""
    if config.get("openai_api_key"):
        return OpenAIEmbeddings(api_key=config.get("openai_api_key"))
    else:
        # Fallback to local model that doesn't require API keys
        return HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

def load_documents(history_dir: str) -> List[Document]:
    """Load all text files from history directory"""
    documents = []
    history_path = Path(history_dir)
    
    if not history_path.exists():
        print(f"Error: Directory '{history_dir}' does not exist")
        sys.exit(1)
    
    # Find all text files recursively
    file_paths = glob.glob(os.path.join(history_dir, "**/*.txt"), recursive=True)
    
    for file_path in file_paths:
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
                
            # Extract metadata from filename
            filename = os.path.basename(file_path)
            file_type = "unknown"
            timestamp = None
            
            # Parse timestamp from filenames like "1234567890-prompt.txt"
            parts = filename.split('-')
            if len(parts) >= 2 and parts[0].isdigit():
                try:
                    timestamp = datetime.fromtimestamp(int(parts[0]))
                    file_type = parts[1].split('.')[0]  # prompt, response, etc.
                except:
                    pass
                
            doc = Document(
                page_content=content,
                metadata={
                    "source": file_path,
                    "filename": filename,
                    "type": file_type,
                    "timestamp": timestamp
                }
            )
            documents.append(doc)
            
        except Exception as e:
            print(f"Error reading {file_path}: {e}")
    
    print(f"Loaded {len(documents)} documents from {history_dir}")
    return documents

def create_or_load_index(documents: List[Document], embeddings, index_name: str = "history_index"):
    """Create or load vector index"""
    # Create chunks for better retrieval
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    chunks = text_splitter.split_documents(documents)
    
    # Check if index already exists
    if os.path.exists(index_name) and os.path.isdir(index_name):
        print(f"Loading existing index from {index_name}")
        try:
            vector_db = FAISS.load_local(index_name, embeddings)
            return vector_db
        except Exception as e:
            print(f"Error loading index: {e}")
            print("Creating new index...")
    
    # Create new index
    print(f"Creating new index with {len(chunks)} chunks")
    vector_db = FAISS.from_documents(chunks, embeddings)
    
    # Save index
    vector_db.save_local(index_name)
    return vector_db

def search_documents(query: str, vector_db, k: int = 5) -> List[Document]:
    """Search for relevant documents"""
    results = vector_db.similarity_search(query, k=k)
    return results

def format_results(results: List[Document]) -> str:
    """Format search results for display"""
    output = []
    
    for i, doc in enumerate(results):
        metadata = doc.metadata
        source = metadata.get("source", "Unknown source")
        timestamp = metadata.get("timestamp")
        doc_type = metadata.get("type", "unknown")
        
        # Format timestamp if available
        time_str = ""
        if timestamp:
            time_str = f" ({timestamp.strftime('%Y-%m-%d %H:%M:%S')})"
        
        # Prepare header
        header = f"\n[{i+1}] {source}{time_str} - Type: {doc_type}"
        output.append(header)
        output.append("-" * len(header))
        
        # Add content preview (truncated if too long)
        content = doc.page_content
        if len(content) > 500:
            content = content[:500] + "..."
        output.append(content)
    
    return "\n".join(output)

def main():
    parser = argparse.ArgumentParser(description="Search through history documents using RAG")
    parser.add_argument("query", help="Search query")
    parser.add_argument("--dir", default="dialogue", help="Directory containing history documents")
    parser.add_argument("--rebuild", action="store_true", help="Force rebuild of the search index")
    parser.add_argument("--results", type=int, default=5, help="Number of results to return")
    args = parser.parse_args()
    
    # Initialize
    config = load_config()
    embeddings = initialize_embeddings(config)
    
    # Load documents
    documents = load_documents(args.dir)
    
    # Create or load vector index
    index_name = "history_index"
    if args.rebuild and os.path.exists(index_name):
        import shutil
        print(f"Rebuilding index - removing {index_name}")
        shutil.rmtree(index_name)
    
    vector_db = create_or_load_index(documents, embeddings, index_name)
    
    # Search for relevant documents
    results = search_documents(args.query, vector_db, k=args.results)
    
    # Format and print results
    if results:
        print(f"\nFound {len(results)} relevant documents for query: '{args.query}'\n")
        formatted_results = format_results(results)
        print(formatted_results)
    else:
        print(f"No relevant documents found for query: '{args.query}'")
    
    return 0

if __name__ == "__main__":
    sys.exit(main())


# Content of patches/1741374768.patch:
<unused for now>


# Content of patches/1741377087.patch:
<unused for now>


# Content of patches/1741374726.patch:
<unused for now>


# Content of patches/1741395755.patch:
<unused for now>


# Content of commands/1741374768.sh:
#!/bin/bash

cat > search_tool.py << 'EOF'
#!/usr/bin/env python3

import argparse
import sys
import urllib.parse


def process_keywords(keywords):
    """
    Process a list of keywords for search.
    
    Args:
        keywords (list): List of keyword strings
        
    Returns:
        str: Processed keywords ready for search
    """
    # Remove any empty strings and join with spaces
    processed = " ".join([k.strip() for k in keywords if k.strip()])
    return processed


def construct_search_url(query):
    """
    Construct a Google search URL from a query string.
    
    Args:
        query (str): Search query string
        
    Returns:
        str: Formatted Google search URL
    """
    encoded_query = urllib.parse.quote(query)
    return f"https://www.google.com/search?q={encoded_query}"


def main():
    """
    Main function to process command line arguments and run the search tool.
    """
    parser = argparse.ArgumentParser(description="Search tool that summarizes Google search results.")
    parser.add_argument(
        "keywords", 
        nargs="+", 
        help="Keywords to search for"
    )
    parser.add_argument(
        "-n", "--num-results",
        type=int,
        default=5,
        help="Number of results to summarize (default: 5)"
    )
    
    args = parser.parse_args()
    
    # Process keywords into a search query
    query = process_keywords(args.keywords)
    search_url = construct_search_url(query)
    
    print(f"Search query: {query}")
    print(f"Search URL: {search_url}")
    print(f"Will summarize the first {args.num_results} results.")
    
    # The actual search and summarization will be implemented in Phase 2 and beyond
    print("Search and summarization functionality will be implemented in future phases.")


if __name__ == "__main__":
    main()
EOF
chmod +x search_tool.py


# Content of commands/1741374726.sh:
#!/bin/bash

echo 'def construct_search_query(keywords):
    query = "+".join(keywords)
    return f"https://www.google.com/search?q={query}"
' > search_tool.py


# Content of commands/1741377087.sh:
#!/bin/bash

cat > search_tool.py << 'EOF'
#!/usr/bin/env python3

import argparse
import sys
import os
import urllib.parse
import tempfile


def process_keywords(keywords):
    """
    Process a list of keywords for search.
    
    Args:
        keywords (list): List of keyword strings
        
    Returns:
        str: Processed keywords ready for search
    """
    # Remove any empty strings and join with spaces
    processed = " ".join([k.strip() for k in keywords if k.strip()])
    return processed


def construct_search_url(query):
    """
    Construct a Google search URL from a query string.
    
    Args:
        query (str): Search query string
        
    Returns:
        str: Formatted Google search URL
    """
    encoded_query = urllib.parse.quote(query)
    return f"https://www.google.com/search?q={encoded_query}"


def create_temp_directory():
    """
    Create a temporary directory to store screenshots.
    
    Returns:
        str: Path to the temporary directory
    """
    temp_dir = tempfile.mkdtemp(prefix="search_tool_")
    print(f"Created temporary directory for screenshots: {temp_dir}", file=sys.stderr)
    return temp_dir


def main():
    """
    Main function to process command line arguments and run the search tool.
    """
    parser = argparse.ArgumentParser(description="Search tool that summarizes Google search results.")
    parser.add_argument(
        "keywords", 
        nargs="+", 
        help="Keywords to search for"
    )
    parser.add_argument(
        "-n", "--num-results",
        type=int,
        default=5,
        help="Number of results to summarize (default: 5)"
    )
    parser.add_argument(
        "-o", "--output-dir",
        default=None,
        help="Directory to save screenshots (default: temporary directory)"
    )
    
    args = parser.parse_args()
    
    # Process keywords into a search query
    query = process_keywords(args.keywords)
    search_url = construct_search_url(query)
    
    # Create output directory if specified, otherwise use a temporary directory
    output_dir = args.output_dir if args.output_dir else create_temp_directory()
    if args.output_dir and not os.path.exists(args.output_dir):
        os.makedirs(args.output_dir)
    
    print(f"Search query: {query}")
    print(f"Search URL: {search_url}")
    print(f"Will summarize the first {args.num_results} results.")
    print(f"Screenshots will be saved to: {output_dir}")
    
    # The actual search, screenshot capture, and summarization will be implemented in Phase 2 and beyond
    print("Search and summarization functionality will be implemented in future phases.")


if __name__ == "__main__":
    main()
EOF
chmod +x search_tool.py


# Content of commands/1741395755.sh:
#!/bin/bash

cat > search_utils.py << 'EOF'
#!/usr/bin/env python3

import os
import sys
import json
import tempfile
import requests
from urllib.parse import quote


def process_keywords(keywords):
    """
    Process a list of keywords for search.
    
    Args:
        keywords (list): List of keyword strings
        
    Returns:
        str: Processed keywords ready for search
    """
    # Remove any empty strings and join with spaces
    processed = " ".join([k.strip() for k in keywords if k.strip()])
    return processed


def construct_search_url(query):
    """
    Construct a Google search URL from a query string.
    
    Args:
        query (str): Search query string
        
    Returns:
        str: Formatted Google search URL
    """
    encoded_query = quote(query)
    return f"https://www.google.com/search?q={encoded_query}"


def create_temp_directory():
    """
    Create a temporary directory to store screenshots.
    
    Returns:
        str: Path to the temporary directory
    """
    temp_dir = tempfile.mkdtemp(prefix="search_tool_")
    print(f"Created temporary directory for screenshots: {temp_dir}", file=sys.stderr)
    return temp_dir


def load_config(config_path="config.json"):
    """
    Load configuration from a JSON file.
    
    Args:
        config_path (str): Path to the config file
        
    Returns:
        dict: Configuration dictionary
    """
    if not os.path.exists(config_path):
        # Check if example config exists and copy it
        example_config = "example.config.json"
        if os.path.exists(example_config):
            print(f"Config file {config_path} not found. Please copy and modify {example_config}", file=sys.stderr)
        else:
            print(f"Config file {config_path} not found.", file=sys.stderr)
        return {}
        
    try:
        with open(config_path, 'r') as f:
            return json.load(f)
    except json.JSONDecodeError:
        print(f"Error parsing config file {config_path}", file=sys.stderr)
        return {}
    except Exception as e:
        print(f"Error loading config file: {e}", file=sys.stderr)
        return {}


def search_google_api(query, api_key, search_engine_id, num_results=5):
    """
    Search Google using the Custom Search JSON API.
    
    Args:
        query (str): Search query string
        api_key (str): Google API key
        search_engine_id (str): Google Custom Search Engine ID
        num_results (int): Number of results to return
        
    Returns:
        list: List of dictionaries containing search results
    """
    base_url = "https://www.googleapis.com/customsearch/v1"
    params = {
        'q': query,
        'key': api_key,
        'cx': search_engine_id,
        'num': min(num_results, 10)  # API allows max 10 results per request
    }
    
    try:
        response = requests.get(base_url, params=params)
        response.raise_for_status()  # Raise exception for HTTP errors
        
        search_results = response.json()
        
        if 'items' not in search_results:
            print("No search results found.", file=sys.stderr)
            return []
        
        results = []
        for item in search_results['items']:
            result = {
                'title': item.get('title', 'No title'),
                'link': item.get('link', ''),
                'snippet': item.get('snippet', 'No snippet available')
            }
            results.append(result)
            
        return results
        
    except requests.exceptions.RequestException as e:
        print(f"Error making API request: {e}", file=sys.stderr)
        return []
    except json.JSONDecodeError:
        print(f"Error parsing API response", file=sys.stderr)
        return []
    except Exception as e:
        print(f"Unexpected error during search: {e}", file=sys.stderr)
        return []
EOF
