```python
ollama_api_url = config.get("ollama_api_url", "http://localhost:11434")  # Default to standard Ollama port
```

Here's the complete implementation to add Ollama support to your composer.py server:

```python
# Add to the @app.route('/api/generate') function

elif provider == 'ollama':
    # Process messages for Ollama format
    ollama_messages = []
    
    # Handling images for Ollama
    has_images = False
    for msg in messages:
        content = msg.get("content", "")
        
        # Check if content is a list (might contain images)
        if isinstance(content, list):
            text_parts = []
            for part in content:
                if isinstance(part, dict):
                    # Handle image_url type content
                    if part.get("type") == "image_url":
                        has_images = True
                        # For now, we'll skip images as Ollama has limited multimodal support
                        # Future: implement image handling for supported models
                        text_parts.append("[Image content not supported in this Ollama model]")
                    # Add other content types as text
                    elif "text" in part:
                        text_parts.append(part["text"])
                elif isinstance(part, str):
                    text_parts.append(part)
            ollama_messages.append({"role": msg["role"], "content": " ".join(text_parts)})
        else:
            # Simple text message
            ollama_messages.append({"role": msg["role"], "content": content})
    
    # Warn if images were skipped
    if has_images:
        print("Warning: Images in messages were skipped for Ollama request")
    
    # Prepare Ollama API request
    ollama_data = {
        "model": model,
        "messages": ollama_messages,
        "options": {
            "temperature": temperature,
            "num_predict": max_tokens
        }
    }
    
    # Send request to Ollama API
    try:
        ollama_response = requests.post(
            f"{ollama_api_url}/api/chat",
            json=ollama_data,
            headers={"Content-Type": "application/json"}
        )
        
        # Handle response
        if ollama_response.status_code == 200:
            response_data = ollama_response.json()
            return jsonify({
                'success': True,
                'content': response_data.get('message', {}).get('content', ''),
                'model': model,
                'provider': 'ollama'
            })
        else:
            return jsonify({
                'success': False,
                'error': f"Ollama API error: {ollama_response.status_code} - {ollama_response.text}"
            }), 500
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f"Error connecting to Ollama API: {str(e)}"
        }), 500
```

Now let's update the health check endpoint to include Ollama:

```python
@app.route('/api/health', methods=['GET'])
def health_check():
    available_providers = []
    if openai_client:
        available_providers.append('openai')
    if anthropic_client:
        available_providers.append('anthropic')
    if openrouter_client:
        available_providers.append('openrouter')
    
    # Check if Ollama is available by making a simple request
    try:
        response = requests.get(f"{ollama_api_url}/api/tags")
        if response.status_code == 200:
            available_providers.append('ollama')
            # Get available models
            models = response.json().get('models', [])
            return jsonify({
                'status': 'ok',
                'available_providers': available_providers,
                'ollama_models': [model.get('name') for model in models]
            })
    except:
        # If Ollama check fails, continue without it
        pass
        
    return jsonify({
        'status': 'ok',
        'available_providers': available_providers
    })
```

Finally, let's update the search_tool.py to include Ollama as a provider option:

```python
def parse_arguments():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(description="Search tool with screenshot capture and LLM summarization")
    parser.add_argument("keywords", nargs="+", help="Keywords to search for")
    parser.add_argument("-n", "--num-results", type=int, default=5, 
                        help="Number of search results to process (default: 5)")
    parser.add_argument("-m", "--model", default="x-ai/grok-2-vision-1212", 
                        help="LLM model to use for summarization")
    parser.add_argument("-p", "--provider", default="openrouter", 
                        choices=["openai", "anthropic", "openrouter", "ollama"],
                        help="AI provider to use")
    parser.add_argument("-s", "--save", action="store_true", 
                        help="Save search results and summaries")
    parser.add_argument("-o", "--output-dir", 
                        help="Directory to save results (default: search_results)")
    return parser.parse_args()
```

This implementation provides full Ollama support with:
1. Configuration for Ollama API URL
2. Message processing for Ollama format
3. API request handling for Ollama
4. Integration with the health check endpoint
5. Updated CLI arguments in search_tool.py

The implementation handles potential image content by providing a placeholder message, as Ollama has limited multimodal support depending on the model you're using.